{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2 align=center> Fine-Tune BERT for Text Classification</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "    <img width=\"512px\" src='https://drive.google.com/uc?id=1fnJTeJs5HUpz7nix-F9E6EZdgUflqyEu' />\n",
        "    <p style=\"text-align: center;color:gray\">Figure 1: BERT Classification Model</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "In this notebook, we will fine tune a BERT base model with Quora data\n",
        "\n",
        "**BERT Model:** https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4 <br>\n",
        "**BERT Data Preprocessor:** https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3 <br>\n",
        "**Dataset:** https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip\n",
        "\n",
        "Here is flow on high level:\n",
        "\n",
        "- #1 - Install and load necessary packages\n",
        "- #2 - Read the data into a pandas dataframe\n",
        "- #3 - Split dataset into train, validation and test data sets\n",
        "- #4 - Create input pipeline using tf.data.dataset\n",
        "- #5 - Load the encoder model and preprocessor model. BERT encoder require input data in fixed format, i.e., input_word_ids, input_mask input_type_ids. We can preprocess using tensorflow/keras tokenizer or use preprocessor model. In this notebook, I have used preprocess model. \n",
        "- #6 - Create the model with:\n",
        "    - Input tensor\n",
        "    - Preprocessor model\n",
        "    - encoder\n",
        "    - dropout\n",
        "    - Final dense with sigmoid activation\n",
        "- #7 - Compile the model, using BinaryCrossEntropy loss and AdamW optimizer. \n",
        "- #8 - Train the model\n",
        "- #9 - Evaluate model against the test dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Install packages</h3>\n",
        "\n",
        "Some packages are required to run this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IxFiq0HVHeNV",
        "outputId": "4f449d7e-7179-4b30-d549-635c971b0944"
      },
      "outputs": [],
      "source": [
        "# https://pypi.python.org/pypi/pydot\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "!pip install tensorflow-text\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow-hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running this notebook on cloud like colab, you will need to restart the runtime. If you are running on local machine then just proceed. \n",
        "Set logger to show only Fatal error, otherwise you will get lots of debug messages. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5Y_aXMQGune"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Load Dataset</h3>\n",
        "\n",
        "I am using quora dataset. This is a labeled dataset on whether the question asked is sincere or unsincere. There are 1.3M rows in the dataset. The data has 3 columns:\n",
        "- GUID: ID for reach row\n",
        "- Question Text: Question asked by user on Quora\n",
        "- Target: Whether question is insincere or not ( 1 = unsincere, 0 = sincere)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "kXxhGA3zGuni",
        "outputId": "fae150d4-b732-43e5-e7a0-34987c363b4d"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip', compression='zip', low_memory=False)\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "df.tail(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Split Dataset</h3>\n",
        "\n",
        "Working on this dataset require some good compute power. To expedite on my macbook and free colab, I have trained only on .5% of dataset. This may not be best choice as it can lead to overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8bkEfehGunk",
        "outputId": "98154dd1-2b47-4f16-82f0-e464911ba9fe"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, random_state=42, train_size=0.005, stratify=df.target.values)\n",
        "train_df, valid_df = train_test_split(train_df, random_state=42, train_size=0.9, stratify=train_df.target.values)\n",
        "print(f'train: {train_df.shape}; valid: {valid_df.shape}; test: {test_df.shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Load Pretrained Models</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXvh2uZPGunk"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000\n",
        "\n",
        "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4', trainable=True, name = 'bert_layer')\n",
        "bert_preprocess_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3', trainable=False, name = 'bert_preprocess_layer')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Input Pipeline</h3>\n",
        "\n",
        "Create a input pipeline with tf.data.dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7cl_eTeGunl",
        "outputId": "30573356-d14d-4872-bb72-53b2c70d909e"
      },
      "outputs": [],
      "source": [
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  train_data = (tf.data.Dataset.from_tensor_slices((train_df.question_text.values, train_df.target.values))\n",
        "  .shuffle(shuffle_buffer_size)\n",
        "  .batch(batch_size, drop_remainder=True)\n",
        "  .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "  valid_data = (tf.data.Dataset.from_tensor_slices((valid_df.question_text.values, valid_df.target.values))\n",
        "  .batch(batch_size, drop_remainder=True)\n",
        "  .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "\n",
        "print(train_data.element_spec)\n",
        "print(valid_data.element_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Preprocessing Sample</h3>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img width=\"512px\" src='https://drive.google.com/uc?id=1-SpKFELnEvBMBqO7h3iypo8q9uUUo96P' />\n",
        "    <p style=\"text-align: center;color:gray\">Figure 2: BERT Tokenizer</p>\n",
        "</div>\n",
        "\n",
        "Review segments created by proprocessor model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl7FKahoGunm",
        "outputId": "e67d5389-8255-4cd4-db02-d7aef0e08ff7"
      },
      "outputs": [],
      "source": [
        "test_text = []\n",
        "for x in train_data.take(1):\n",
        "    for i in range(1):\n",
        "        test_text.append(x[0][i].numpy())\n",
        "\n",
        "print(test_text)\n",
        "text_processed = bert_preprocess_layer(test_text)\n",
        "print(text_processed.keys())\n",
        "print(f\"input_word_id:: shape:{text_processed['input_word_ids'].shape}; values:{text_processed['input_word_ids']}\")\n",
        "print(f\"input_mask:: shape:{text_processed['input_mask'].shape}; values:{text_processed['input_mask']}\")\n",
        "print(f\"input_type_ids:: shape:{text_processed['input_type_ids'].shape}; values:{text_processed['input_type_ids']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Create Model</h3>\n",
        "\n",
        "Create classification model with following layers\n",
        "- Text Input\n",
        "- Preprocessor layer\n",
        "- Encoder\n",
        "- Dropout\n",
        "- Dense with sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkZiR8IpGunm"
      },
      "outputs": [],
      "source": [
        "# Building the model\n",
        "def create_model():\n",
        "\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='Input')\n",
        "  encoder_inputs = bert_preprocess_layer(text_input)\n",
        "  pooled_output = bert_layer(encoder_inputs)['pooled_output']\n",
        "  drop = tf.keras.layers.Dropout(0.1)(pooled_output)\n",
        "  output = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(drop)\n",
        "  return tf.keras.Model(text_input, output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Compile Model</h3>\n",
        "\n",
        "Compile model with BinaryCrossEntropy loss and AdamW optimizer. Notice we are using AdamW optimizer, not Adam. BERT was pretrained using AdamW. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkY_RJAcGunn",
        "outputId": "074a91bc-2a8b-4aaf-ba41-09b7ae5a169f"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = create_model()\n",
        "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=2e-5),\n",
        "              loss=tf.keras.losses.BinaryFocalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "yK5RGbb2Guno",
        "outputId": "64c7e885-d974-48cc-ae68-2e66d76f1176"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model=model, show_shapes=True, dpi=76, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Train Model</h3>\n",
        "\n",
        "Train the model. I ran it for 4 epochs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q6NsxhwGuno",
        "outputId": "0fb3ed10-097a-4a32-e1f3-977bca7b0f2f"
      },
      "outputs": [],
      "source": [
        "epochs = 4\n",
        "history = model.fit(train_data,\n",
        "                    validation_data=valid_data,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot loss and accuracy of both training and validation set. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "Ta7rGS71Mnpr",
        "outputId": "620c7593-6b13-4014-dcdb-97e7dcf1500a"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# r is for \"solid red line\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Evaluate</h3>\n",
        "\n",
        "Evaluate the model with test dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "2MuqflhFNS9V",
        "outputId": "6527576a-92ec-4383-fc1c-bdad5ea00f70"
      },
      "outputs": [],
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  test_data = (tf.data.Dataset.from_tensor_slices((test_df.question_text.values, test_df.target.values))\n",
        "  .batch(batch_size, drop_remainder=True)\n",
        "  .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "\n",
        "loss, accuracy = model.evaluate(test_data)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<H3>Acnowledgement</h3>\n",
        "\n",
        "This notebook was created based on guided project on Coursera at https://www.coursera.org/learn/fine-tune-bert-tensorflow/ungradedLti/ack5t/fine-tune-bert-for-text-classification-with-tensorflow. I modified it to take advantage of preprocessor model and latest version of tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<H3>Next Steps</h3>\n",
        "\n",
        "This is very basic setup. You might want to try with different values for data split, dropout, learning rate, etc. As they say, you can only train with experimentation, there is no one value for any of these. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
